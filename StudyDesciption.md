The best way to look through this table is to select the line of interest and click the mouse button to move it to the right or left. 
For a better overview, I recommend using the Excel table with the same name.

| Authors Information |   |   |   | System Information |   |  |  |   |   | Describing Parameters of the Algorithm |   |   | Characteristics of the Empirical Study |   |   |   |
|---------------------|---|---|---|--------------------|---|--|--|---|---|----------------------------------------|---|---|----------------------------------------|---|---|---|
| Authors                                                                                                                                                                                           | Year of Publication | Titel                                                                                                                                                                                      | Doi                                                 | Technology overview                                                                                                                                                       | Eye movement detection Device, Manufacturer                                |Eye Tracking Technique and Sensors   | Type of Fixation at the eye | Dimension of the robot manipulation space | Type of robot, Manufacturer                        | Algorithms and models used in the approach                                                                                                                                                                                             | Programmed with …                                                                                   | Type of Eye movement used to control the system                             | Task description                                                                                                                                                                                                                                                                                | Number of Participants                                                                  | Number of Repetitions        | Performed empirical Test                                                                                                             |
|------------------------------------------|---|---------------------------------------------|---|-----------------------------------------|------------------------|--------------------------------|------------------------|------------------------|------------------------|-------------------------------------------------------------|---------------------------------------------|---------------------------------------------|----------------------------------------------------------------------------------|------------------------|---|---|
| Alsharif, S., Kuzmicheva, O., and Gräser, A.                                                                                                                                                      | 2016                | Gaze Gesture-Based Human Robot Interface                                                                                                                                                   | -                                                   | Eye tracking controlling a collaborative robot.                                                                                                                           | SMI eye tracking glasses, SensoMotoric Instruments GmbH                    |Dark Pupil [1], sensors not specified            | Head-Worn                   | 3D                                        | LWA 3, Schunk; gripper PG70, Schunk                | TFST (Timed Finite State Machine), Normalized Cross Correlation (NCC)                                                                                                                                                                  | iViewNGTM SDK (to detect gaze direction), ARToolKitPlus SDK (to determine gaze position to markers) | Saccades and Fixations                                                      | Two columns were presented Two subtasks were executed. The user should control the robot to grasp the first cube from the first column and put it on the second column behind it (first subtask), then grasp the other cube from the third column and put it on the first one (second subtask). | 10,of which 1 is diagnosed with multiple sclerosis and is paralyzed from the neck down. | -                            | Nasa TLX                                                                                                                             |
| Bannat, A., gast, J., Rehrl, T., Rösel, W., Rigoll, G., and Wallhoff, F.                                                                                                                          | 2009                | A Multimodal Human-Robot-Interaction Scenario: Working Together with an Industrial Robot                                                                                                   | https://doi.org/10.1007/978-3-642-02577-8_33        | Eyetracking, Speech recognition and Touch screen are used to control an industrial robot, mobilie platform and IoT.                                                       | EyeSeeCam, EyeSeeTec GmbH                                                  || Head-worn                   | 3D                                        | Mitsubishi robot RV-6SL., Mitsubishi               | -                                                                                                                                                                                                                                      | -                                                                                                   | Fixation                                                                    | No User Study was conducted.                                                                                                                                                                                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Bien, Z., Kim, D., Chung, M., Kwon, D., and Chang, P.                                                                                                                                             | 2003                | Development of a wheelchair-based rehabilitation robotic system (KARES II) with various human-robot interaction interfaces for the disabled                                                | https://doi.org/10.1109/AIM.2003.1225462            | Eyetracking, EMG, haptic suit and a display are used to control a robot arm and a wheelchair. Further, mouth recognition is implemented for sensoring readyness to drink. | Self-developedCCD camera-based wearable system                             |Dark Eye Effect (Note from Author: today - Dark pupil tracking), CCD camera with IR LED illumination| Head-worn                   | -                                         | Self developed arm (6 DoF) PUMA Type               | Self written: Gabor-filter based Gaussian weighted feature for mouth recognition, modified log-polar mapping to detect near motion, Fuzzy min max neural networks (FMMNN)-based classification for shouledr/head movement recognition. |  -                                                                                                  | -                                                                           | No User Study was conducted.                                                                                                                                                                                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Bien, Z., Chung, M., Chang, P., Kwon, D., Kim, D., Han, J., Kim, J., Kim, D., Park, H., Kang, S., Lee, K., and Lim, S.                                                                            | 2004                | Integration of a Rehabilitation Robotic System (KARES II) with Human-Friendly Man-Machine Interaction Units                                                                                | https://doi.org/10.1023/B:AURO.0000016864.12513.77  | Eye tracking (eye-mouse), EMG, shoulder/head interface and a display are used to control a robot arm and a mobile platform. Further, face recognition is implemented.     | Eye-mouse interface (self developed, fixed on a cap for user friendliness) |Dark Eye Effect (Note from Author: today - Dark pupil tracking), CCD camera with IR LED illumination| head-worn                   | 3D                                        | Self developed arm (6 DoF) PUMA Type               | Self written: Gabor-filter based Gaussian weighted feature for mouth recognition, modified log-polar mapping to detect near motion, Fuzzy min max neural networks (FMMNN)-based classification for shouledr/head movement recognition. | -                                                                                                   | Fixation                                                                    | The experiment tested the performance of the system in a drinking task.                                                                                                                                                                                                                         | 6 impaired participants with C4/c5 lesions                                              | -                            | Satisfaction degree in percent in form of a questionnaire                                                                            |
| Catalán, J.M., Díez, J.A., Bertomeu-Motos, A., Badesa, F.J., and Garcia-Aracil, N.                                                                                                                | 2017                | Multimodal Control Architecture for Assistive Robotics                                                                                                                                     | http://dx.doi.org/10.1007/978-3-319-46669-9_85      | Eye tracking is used to control an assistive robotic arm. Objects were tracked by an MOTIVE system (Optitrack).                                                           | Tobii Pro Glasses 2 , Tobii                                                |Dark pupil tracking [3]: Eye tracking sensors with IR illuminators| Head-Worn                   | 3D                                        | JACO, Kinova                                       | -                                                                                                                                                                                                                                      | ROS, MOTIVE Optitrack                                                                               | Fixation, Dwell Time optimized to Fitts' Law                                | No User Study was conducted.                                                                                                                                                                                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Cio, Y.L, Raison, M., Leblond Menard, C., and Achiche, S.                                                                                                                                         | 2019                | Proof of Concept of an Assistive Robotic Arm Control Using Artificial Stereovision and Eye-Tracking                                                                                        | https://doi.org/10.1109/TNSRE.2019.2950619          | Eye tracking is used to control an assistive robotic arm.                                                                                                                 | Eye Tribe                                                                  |Pupil with corneal reflection, camera [4], Headtracking| Remote                      | 3D                                        | JACO, Kinova                                       | A*-Algorithm (for pathplanning and Collision-Avoidance)                                                                                                                                                                                | GraspIt (ROS, for Grasping), OpenCV (for 3D Scene aquisition)                                       | Fixation                                                                    | No User Study was conducted.                                                                                                                                                                                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Di Maio, M., Dondi, P., Lombardi, L., and Porta, M.                                                                                                                                               | 2021                | Hybrid Manual and Gaze-Based Interaction With a Robotic Arm                                                                                                                                | https://doi.org/10.1109/ETFA45728.2021.9613371      | Eye tracking and computer mouse are used to control an assistive robotic arm over a display.                                                                              | Tobii 4C, Tobii                                                            |Switchable between dark and bright pupil detection, Near Infrared illuminators [7, 5]| Remote                      | 3D                                        | IRB 4600, ABB (Simulation)                         | -                                                                                                                                                                                                                                      | Tobii Core SDK.                                                                                     | Fixation                                                                    | The participants were asked to execute joint movemnts by using their gaze.                                                                                                                                                                                                                      | 7 participants                                                                          | -                            | SUS                                                                                                                                  |
| Dragomir, A., Pana, C.F., Cojocaru, D., and Manga, L.F.                                                                                                                                           | 2021                | Human-Machine Interface for Controlling a Light Robotic Arm by Persons with Special Needs                                                                                                  | http://dx.doi.org/10.1109/ICCC51557.2021.9454664    | Eyetracking is used to control a collaborative robot and electric wheelchair over a display.                                                                              | -                                                                          |Eye Tracker type not specified.| Remote                      | 3D                                        | Gen3, Kinova                                       | -                                                                                                                                                                                                                                      | -                                                                                                   | Fixation                                                                    | No User Study was conducted.                                                                                                                                                                                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Dziemian, S., Abbott, W.W., and Faisal, A.A.                                                                                                                                                      | 2016                | Gaze-based teleprosthetic enables intuitive continuous control of complex robot arm use: Writing & drawing                                                                                 | https://doi.org/10.1109/BIOROB.2016.7523807         | Eye tracking is used to control a industrial robot arm. Head tracking is used to control the robot in the third direction (moving forward and backward).                  | Tobii Eye X Controller, Tobii                                              |Pupil center/corneal reflection and dark pupil tracking depending on the measured parameter, Backlight Assisted Near Infrared illuminators [6, 5]| Remote                      | 3D                                        | UR10, Universal Robots                             | -                                                                                                                                                                                                                                      | moveit ROS                                                                                          | Fixation and Wink                                                           | The task contained tele-writing or painting: Participants were asked to imagine writing a text with the pen and look where the pen would be going. They were asked to write letters as fast and as accurate as possible, with a given letter size template.                                     | 8 Subjects                                                                              | 5                            | -                                                                                                                                    |
| Huang, Q., Zhang, Z., Yu, T., He, S., and Li, Y.                                                                                                                                                  | 2019                | An EEG-/EOG-Based Hybrid Brain-Computer Interface: Application on Controlling an Integrated Wheelchair Robotic Arm System                                                                  | https://doi.org/10.3389/fnins.2019.01243            | hBCI, EEG and EOG is used to control an assistive robot arm over a display. Cameras are used to realize a shared control.                                                 | -                                                                          |EEG and EOG type not specified, motiontrcking with Kinect v2, Microsoft. 10-20 electrodes used| Head-worn                   | 3D                                        | JACO, Kinova                                       | Description of EEG Signal processing                                                                                                                                                                                                   | Matlab for accuracy calculation                                                                     | eye blinks and eyebrow raise. Addition control over hand motor imagery      | The study thested a self-drinking task. Participants were asked to move a wheelchair to a table (EEG), manipulating the robotic arm via EOG to grasp a bottle and drink with a straw, placing the bottle back and navigating back through multiple obstacles and a door.                        | 22 participants in total, 5 participants in the self drinking task (robot manipulation) | 3                            | -                                                                                                                                    |
| Huang, C., and Mutlu, B.                                                                                                                                                                          | 2016                | Anticipatory robot control for efficient human-robot collaboration                                                                                                                         | https://doi.org/10.1109/HRI.2016.7451737            | Eye tracking and speech recognition are used to control a robot arm.                                                                                                      | SMI eye tracking glasses, SensoMotoric Instruments GmbH                    |Dark Pupil [1], sensors not specified| Head-Worn                   | -                                         | MICO, Kinova                                       | Anticipatory motion planning                                                                                                                                                                                                           | MoveIt! ROS                                                                                         | -                                                                           | Participants had to choose a smoothie out of 12 ingredients. The robot system has to anticipate the choices by the eye gaze and grasp the right item and place it infront of the user.                                                                                                          | 26 participants                                                                         | -                            | Questionnaire for perceived awareness and intentionality of the robot.                                                               |
| Iáñez, E., Azorín, J.M., Fernández, E., and Úbeda, A.                                                                                                                                             | 2010                | Interface based on electrooculography for velocity control of a robot arm                                                                                                                  | https://doi.org/10.1080/11762322.2010.503107        | EOG is used to control a robot over a display.                                                                                                                            | NeuroScan, manifacturer not specified                                      |Five electrodes were placed for requiring horizontal and vertical EOG signals. | head-worn                   | 2D                                        | LR Mate 200iB, FANUC                               | -                                                                                                                                                                                                                                      | API from Matlab, C++ application                                                                    | Fixation for velocity control and direction, and Blink for target selection | Moving the robot over certain targets.                                                                                                                                                                                                                                                          | 3 participants                                                                          | -                            | -                                                                                                                                    |
| Ivorra, E., Ortega, M., Catalán, J.M., Ezquerro, S., Lledó, L.D., Garcia-Aracil, N., and Alcañiz, M.                                                                                              | 2018                | Intelligent Multimodal Framework for Human Assistive Robotics Based on Computer Vision Algorithms                                                                                          | https://doi.org/10.3390/s18082408                   | EEG and EOG are used to control an assisitve robotic arm. Additionally head and mouth tracking is used to accomplish a drinking task.                                     | Tobii Glasses, Tobii                                                       |Dark pupil tracking: Eye tracking sensors with IR illuminators| Head-Worn                   | 3D                                        | JACO2, Kinova                                      | YOLOV2 was trained with the COCO image database. LINEMOD for Detection and Pose estimation                                                                                                                                             | -                                                                                                   | Fixation                                                                    | The participants were asked to select three kinds of objects by gaze (a glass, a bottle, and a fork) wearing the Tobii Glasses.                                                                                                                                                                 | 10 participants                                                                         | 20                           | -                                                                                                                                    |
| Jones, E., Chinthammit, W., Huang, W., Engelke, U., and Lueg, C.                                                                                                                                  | 2018                | Symmetric Evaluation of Multimodal Human–Robot Interaction with Gaze and Standard Control                                                                                                  | https://doi.org/10.3390/sym10120680                 | Eye tracking is used to control a research robot over a display.                                                                                                          | Eye Tacker, TheEyeTribe                                                    |Pupil with corneal reflection, camera [4]| Remote                      | 2D                                        | WidowX Robot Arm, TrossenRobotics                  | -                                                                                                                                                                                                                                      | All these devices were integrated through a MATLAB interface.                                       | Fixation                                                                    | Participants were asked to play chess in three different difficulties (number of moves, number of figures), and 3 different modalities (eye tracking, controller, multimodal (combined))                                                                                                        | 20 participants                                                                         | 3                            | NASA TLX, together with 5 point Likert scale.                                                                                        |
| Khan, A., Memon, M.A., Jat, Y., and Khan, A.                                                                                                                                                      | 2012                | Electro-Occulogram Based Interactive Robotic Arm Interface for Partially Paralytic Patients                                                                                                | -                                                   | EOG is used to control a robot.                                                                                                                                           | RHA1016, Eye Sense                                                         |Phototranduction [8], Horizontal EOG, five electrodes were placed around the eye. | Head worn                   | 3D                                        | -                                                  | -                                                                                                                                                                                                                                      | -                                                                                                   | Saccades                                                                    | No User Study was conducted.                                                                                                                                                                                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Kim, D.H., Kim, J.H., Yoo, D.H., Lee, Y.J., and Chung, M.J.                                                                                                                                       | 2001                | A Human-Robot Interface Using Eye-Gaze Tracking System for People with Motor Disabilities                                                                                                  | -                                                   | Eye Tracking and Motion Tracking was used to contrl a robot.                                                                                                              | Selfmade CCD camera                                                        |Limbus tracking with an additional iris boundary model, CCD camera and a mirror.| Head-worn                   | 3D                                        | -                                                  | -                                                                                                                                                                                                                                      | freecell                                                                                            | Fixation                                                                    | The participant were asked to push buttons by gazing on them, yet the study did represent a proof of concept. No descriptions on participants were given. Other tests with playing FreeCell.                                                                                                    | -                                                                                       | -                            | -                                                                                                                                    |
| Li, S., Zhang, X., and Webb, J.D.                                                                                                                                                                 | 2017                | 3-D-Gaze-Based Robotic Grasping Through Mimicking Human Visuomotor Function for People With Motion Impairments                                                                             | https://doi.org/10.1109/TBME.2017.2677902           | Eye tracking was used to control an assistive robotic arm.                                                                                                                | not specified                                                              |Dark pupil detection. Image sensor with four near infrared (IR) LEDs illuminators| Head-worn (seen in figures) | 3D                                        | MICO, Kinova                                       | Two stages of graspplanning: 1. sliding window filter to locate object and ist pose 2. Gaussian Mixture Model (GMM) to generate visuomotor grasping model.                                                                             | Robotic Operation System (ROS)                                                                      | Fixation                                                                    | 2 staged grasping approach by fixating locations on the object. The evaluation was quantified from two aspects: 1) the success rate of the grasping task and 2) subjective evaluation using questionnaires.                                                                                     | -                                                                                       | >4                           | USE Questionnair                                                                                                                     |
| McMullen, D.P., Hotson, G., Katyal, K.D., Wester, B.A., Fifer, M.S., McGee, T.G., Harris, A., Johannes, M.S., Vogelstein, R.J., Ravitz, A.D., Anderson, W.S., Thakor, N.V., and Crone, N.E.       | 2014                | Demonstration of a semi-autonomous hybrid brain-machine interface using human intracranial EEG, eye tracking, and computer vision to control a robotic upper limb prosthetic               | https://doi.org/10.1109/tnsre.2013.2294685          | hybrid BCI (EEG/Eye tracking)                                                                                                                                             | NeuroPort System, BlackRock Microsystems, Tobii PCEye, Tobii Technology    |EEG-based approach, Number of elecrodes varied between participants. Method for Tobii PCEye not specified, IR-camera [9]| Remote                      | 3D                                        | MPL modular Prosthetic limb, not further specified | self written, computer vision algorithms and EEG signal processing was used. Point Cloud Library was used                                                                                                                              | ROS, Matlab GUI                                                                                     | Fixation                                                                    | Participants were asked to conduct a reach-grasp-and-drop task, with 3 different balls. It was shown in an objective measurement that up to 8 objects could be detected by the computer vision algorithm with eye tracking.                                                                     | 2 participants                                                                          | 28 and 31 regarding the task | -                                                                                                                                    |
| Onose, G., Grozea, C., Anghelescu, A., Daia, C., Sinescu, C. J., Ciurea, A. V., Spircu, T., Mirea, A., Andone, I., Spânu, A., Popescu, C., Mihăescu, A-S, Fazli, S., Danóczy, M., and Popescu, F. | 2012                | On the feasibility of using motor imagery EEG-based brain-computer interface in chronic tetraplegics for assistive robotic arm control: a clinical test and long-term post-trial follow-up | https://doi.org/10.1038/sc.2012.14                  | EEG-BCI, head tracking and eye tracking were used to control an assistive robotic arm. Visual cues and feedback were given over a display.                                | BrainAmp128 DC, View Point Eye Tracker, Arrington Research                 |Six dry-cap electrodes for EEG signals placed to record signals from the motor cortex. Eye tracking: dark pupil and bright pupil detection, Camera wir IR-LED illumination [10]| Head-worn                   | 3D                                        | Assistive Robotic Manipulator, Exact Dynamics      | -                                                                                                                                                                                                                                      | -                                                                                                   | Fixation                                                                    | Participants were told to visually focus on a glass and activate the BCI system by performing the agreed-to ‘grab’ class (the first of the feedback class pair). After the robot grab action sequence was completed, they were instructed to place the glass back.                              | 9 chronic impaired participants                                                         | -                            | Empiric evaluation of the health status of each patient via sensory AIS score. Clinical Questionaire about perception of the system. |
| Park, K., Choi, S.H., Moon, H., Lee, J.Y., Ghasemi, Y., and Jeong, H.                                                         | 2022 | Indirect Robot Manipulation using Eye Gazing and Head Movement For Future of Work in Mixed Reality                   | https://doi.org/10.1109/VRW55335.2022.00107                                                                                                           | Augemented Reality Glasses were used to control an industrial robot.                                                                                                        | Smart MR Glasses, not specified                                     |Eye Tracking method not specified| Head-worn  | 3D | -                                                       | RetinaNet                                                                                                                                                                                                                                                  | -                                                                                                        | Fixation to search for the object and Head movement to select the object | No User Study was conducted.                                                                                                                                                                                                                                                                                                                                    | -                                                                                              | -                                                         | -                                                    |
| Perez Reynoso, F.D., Niño Suarez, P.A., Aviles Sanchez, O.F., Calva Yañez, M.B., Vega Alvarado, E., and Portilla Flores, E.A. | 2020 | A Custom EOG-Based HMI Using Neural Network Modeling to Real-Time for the Trajectory Tracking of a Manipulator Robot | https://doi.org/10.3389/fnbot.2020.578834                                                                                                             | EOG was used to control a robot.                                                                                                                                            | -                                                                   |Six silver/silver chloride electrodes were placed on the skin and the glasses. | head -worn | 3D | antropomorphic robot, not specified                     | Fuzzy inference system, EOG modeled by means of a MNN, implementing descending backpropagation using Widrow-Hoff technique. Objective function is obtained by means of the neural network, optimized by genetic algorithms. Gaussian membershipm functions | -                                                                                                        | Fixation                                                                 | Experiement 1: Standard calibration with inexpert and expert users, Experiment 2: Customized Calibration With Inexpert Users                                                                                                                                                                                                                                    | 30 participants (experiment 1) and 30 participants (Experiment2)                               | 20                                                        | -                                                    |
| Rusydi, M., Okamoto, T., Ito, S., and Sasaki, M.                                                                              | 2014 | Rotation Matrix to Operate a Robot Manipulator for 2D Analog Tracking Objects Using Electrooculography               | https://doi.org/10.3390/robotics3030289                                                                                                               | EOG was used to control a robot over a display.                                                                                                                             | NF Instrument EOG, not specified                                    |Four electrodes were placed around the eye and the forehead for horizontal and vertical EOG.| Head-Worn  | 2D | 2DoF self build planar robot                            | Rotational matrice for EOG-signals                                                                                                                                                                                                                         | -                                                                                                        | Fixation of dots                                                         | The participants were told to focus 20 different points on a display.                                                                                                                                                                                                                                                                                           | 3 participants                                                                                 | 20                                                        | -                                                    |
| Rusydi, M.I., Sasaki, M., and Ito, S.                                                                                         | 2014 | Affine transform to reform pixel coordinates of EOG signals for controlling robot manipulators using gaze motions    | https://doi.org/10.3390/s140610107                                                                                                                    | EOG was used to control a Robot. Two plates were used as a visual target.                                                                                                   | EOG instrument, not specified                                       |Four electrodes were placed around the eye and the forehead| Head-Worn  | 2D | -                                                       | self written: Rotation and translation matrices                                                                                                                                                                                                            | -                                                                                                        | Fixation                                                                 | The participants were told to fixate 24 visual markers. After training they conducted certain germetrical patters to evaluate the mathematical model.                                                                                                                                                                                                           | 3 participants (operators)                                                                     | -                                                         | -                                                    |
| Scalera, L., Seriani, S., Gallina, P., Lentini, M., and Gasparetto, A.                                                        | 2021 | Human–Robot Interaction through Eye Tracking for Artistic Drawing                                                    | https://doi.org/10.3390/robotics10020054                                                                                                              | Eye tracking was used to control a Robot over a Display.                                                                                                                    | Tobii Eye Tracker 4C, Tobii                                         |Switchable between dark and bright pupil detection, Near Infrared illuminators [7, 5]| Remote     | 3D | UR5, Universal Robotics                                 | a moving average filter, and an interpolation with B-spline curves is used for filtering the gaze positions and segment filtering                                                                                                                          | Matlab                                                                                                   | Saccade and fixation                                                     | No User Study was conducted.                                                                                                                                                                                                                                                                                                                                    | -                                                                                              | -                                                         | -                                                    |
| Scalera, L., Seriani, S., Gasparetto, A., and Gallina, P.                                                                     | 2021 | A novel robotic system for painting with eyes                                                                        | http://dx.doi.org/10.1007/978-3-030-55807-9_22                                                                                                        | Eye tracking was used to control a Robot over a Display.                                                                                                                    | Tobii Eye Tracker 4C, Tobii                                         |Switchable between dark and bright pupil detection, Near Infrared illuminators [7, 5]| Remote     | 3D | UR10, Universal Robotics                                | a moving average filter, and an interpolation with B-spline curves is used for filtering the gaze positions and segment filtering                                                                                                                          | Matlab                                                                                                   | Saccade and Fixation                                                     | The participants were asked to draw with their eyes on basis of AI-generated pictures. The eye movements were interpreted in two ways. While focussing on a location the participant could paint a point, when saccades were performed a line was drawn.                                                                                                        | -                                                                                              | -                                                         | -                                                    |
| Sharma, V.K., Saluja, K., Mollyn, V., and Biswas, P.                                                                          | 2020 | Eye Gaze Controlled Robotic Arm for Persons with Severe Speech and Motor Impairment                                  | https://doi.org/10.1145/3379155.3391324                                                                                                               | Eye tracking was used to control a Robot over a Display.                                                                                                                    | Tobii PCEyeMini tracker, Tobii                                      |Method not specified, IR-camera [9]| Remote     | 3D | Curious Arm, Kit4Curious                                | CNN for face channel, AGE-Net architecture. records gaze coordinates and uses a median filter and Bezier curve to smoothly move a cursor based on eye gaze movement [Biswas 2016]. Computer vision algorithms (not specified)                              | -                                                                                                        | Fixation                                                                 | Participants were asked to pick and drop a Badminton shuttlecock. In the first study the participants could bring the robotic arm at any random point within the field of reach of the robotic arm. In the second task directional arrows were given to move the robot.                                                                                         | Study 1: In total 18 participants (9 healthy, 9 impaired), Study 2: 12 (6 healthy, 6 impaired) | 2 repetitions in each trial                               | -                                                    |
| Sharma, V.K., Murthy, L. R. D., and Biswas, P.                                                                                | 2022 | Comparing Two Safe Distance Maintenance Algorithms for a Gaze-Controlled HRI Involving Users with SSMI               | https://doi.org/10.1145/3530822                                                                                                                       | Eye tracking was used to control a Robot over a Display.                                                                                                                    | Laptop Webcam                                                       |Eye Tracking Method not specified, RBG-D camera| Remote     | -  | Dobot Magician, Variobotic                              | Hand detection (google) to avoid danger to users.AGE-Net architecture for calculating degrees of visual angle of accuracy for MPI-IGaze and RT-GENE datasets with nearly 108M parameters.                                                                  | Markov decision process (MDP) avoid hand collision                                                       | Fixation, Dwell-time on buttons (500ms)                                  | The participant was asked to perform the task of reaching the designated target for a print on cloths twice. The target positions were randomized for each trial and participant.                                                                                                                                                                               | 13 participants (7 healthy, 6 disabled)                                                        | -                                                         | -                                                    |
| Stalljann, S., Wöhle, L., Schäfer, J., and Gebhard, M.                                                                        | 2020 | Performance Analysis of a Head and Eye Motion-Based Control Interface for Assistive Robots                           | https://doi.org/10.3390/s20247162                                                                                                                     | Eye tracking and head tracking with MARG were seperately used to control a industrial robot over a display.                                                                 | Monocular eye tracker headset Pupil Core, Pupil Lab                 |Dark pupil detection with 3D model [11], Eye camera with IR illumination [12].| Head-worn  | 3D | UR5, Univeral Robots; gripper 2F-85, Robotiq            | Authors referenced to other literature for details.                                                                                                                                                                                                        | -                                                                                                        | Saccades and Fixations                                                   | The participants were asked to perform abutton activation task to assess discrete control (event-based control) and a Fitts’s Law task. The usability study was related to a use-case scenario with a collaborative robot assisting a drinking action.                                                                                                          | 10 participants +1 ALS patient                                                                 | 3 Tests, each participant had to reach 33 correct trials. | NASA TLX (RTLX Version) and subjective questionnaire |
| Sunny, Md S.H., Zarif, Md I.I., Rulik, I., Sanjuan, J., Rahman, M.H., Ahamed, S.I., Wang, I., Schultz, K., and Brahmi, B.     | 2021 | Eye-Gaze Control of a Wheelchair Mounted 6DOF Assistive Robot for Activities of Daily Living                         | http://dx.doi.org/10.21203/rs.3.rs-829261/v1                                                                                                          | Eye tracking was used to control a power wheelchair and a industrial robot over a display.                                                                                  | PCEye5, Tobii                                                       |Method not specified, IR-camera [9]| remote     | 3D | xArm 6, UFactory                                        | Denavit-Hartemberg and inverse Kinematic for robot control                                                                                                                                                                                                 | PyQt5 for virtual buttons on graphical user interface                                                    | Fixation                                                                 | Participants were asked to perform activities of daily living (ADL), which included picking objects from the upper shelf, pick an object from a table, picking an object from the ground.                                                                                                                                                                       | 10 participants                                                                                | 5 trials and 3 manipulation tasks                         | Not standardized test                                |
| Tostado, P.M., Abbott, W.W., and Faisal, A.A.                                                                                 | 2016 | 3D gaze Cursor: continous calibration and end-point grasp control of robotic acuators                                | https://doi.org/10.1109/ICRA.2016.7487502                                                                                                             | Eye tracking was used to control a research robot.                                                                                                                          | GT3D binocular eye-tracker, not specified                           |Pupil is detected by using the provided Algorithm of OpenCV [15], Custom build from 2 Playstation3 cameras and 2 IR LEDs| Remote     | 3D | WidowX Robot Arm, TrossenRobotics                       | OpenCV,Gaussian Processes (GP) Regression for calibration                                                                                                                                                                                                  | C++ Interfaces, to connect everything, Matlab gaussian processes regression for machine learning toolbox | Fixation and Wink                                                        | Participants were told to execute a reaching and grasping task with the robot.                                                                                                                                                                                                                                                                                  | 7 participants                                                                                 | -                                                         | Measurements to cognitive load, not specified        |
| Ubeda, A., Iañez, E., and Azorin, J.M.                                                                                        | 2011 | Wireless and Portable EOG-Based Interface for Assisting Disabled People                                              | https://doi.org/10.1109/TMECH.2011.2160354                                                                                                            | EOG was used to control a robot arm. A display (Interface) was used to start and stop the application in the test.                                                          | Self build EOG                                                      |Five dry electrodes  are placed around the eye (one for reference) for horizontal and vertical EOG| Head-worn  | 2D | LRMate 200iB, Fanuc                                     | -                                                                                                                                                                                                                                                          | -                                                                                                        | Rapid Saccades                                                           | Participants were asked to perform trajectories between fixed visual markers by "drawing" these trajectories with their eyes.                                                                                                                                                                                                                                   | 6 participants                                                                                 | -                                                         | -                                                    |
| Wang, Y., Xu, G., Song, A., Xu, B., Li, H., Hu, C., and Zeng, H.                                                              | 2018 | Continuous Shared Control for Robotic Arm Reaching Driven by a Hybrid Gaze-Brain Machine Interface                   | -                                                                                                                                                     | Hybrid BCI consisting of EEG and Eye Tracking was used to control an educational robot arm over a display.                                                                  | EyeX, Tobii AB Inc.                                                 |Pupil center/corneal reflection and dark pupil tracking depending on the measured parameter, Backlight Assisted Near Infrared illuminators [6, 5]| Remote     | 2D | Dobot Arm, Shenzhen Yuejiang Technology Co Inc.         | -                                                                                                                                                                                                                                                          | POpenVibe Toolbox for BMI, OpenCV                                                                        | -                                                                        | Participants were asked to accomplish a reach task with eye tracking and BCI. In this study three paradigms were tested, which could be controlled by the participant: 1) The system with shared control both in speed and direction 2) with shared control in speed only 3) with shared control in direction only.                                             | 6 participants                                                                                 | 3 paradigms each 10 trials                                | -                                                    |
| Wang, H., Dong, X., Chen, Z., and Shi, B.E.                                                                                   | 2015 | Hybrid gaze/EEG brain computer interface for robot arm control on a pick and place task                              | https://doi.org/10.1109/EMBC.2015.7318649                                                                                                             | Hybrid BCI consisting of EEG and Eye Tracking was used to control a robot arm over a display.                                                                               | Tobii X60, Tobii                                                    |Dark Pupil, Bright Pupil detection: Modes are switchable to create better outcomes for certain measurements.| remote     | -  | -                                                       | "Two-stage model of gaze trajectories based on HMM, Euclidean cluster                                                                                                                                                                                      |                                                                                                          |                                                                          |                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                |                                                           |                                                      |
| extraction method (for locating the object)"                                                                                  | -    | Fixation: 100ms Dwell-time                                                                                           | The Participants were asked to sort two objects colored red and blue in their individual colored spaces by using motor imagery to control the robot.  | 8 participants                                                                                                                                                              | -                                                                   |Pupil center corneal reflection (PCCR) [13], Camera and IR illuminators [14]| -          |    |                                                         |                                                                                                                                                                                                                                                            |                                                                                                          |                                                                          |                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                |                                                           |                                                      |
| Webb, J.D., Li, S., and Zhang, X.                                                                                             | 2016 | Using Visuomotor Tendencies to Increase Control Performance in Teleoperation                                         | https://doi.org/10.1109/ACC.2016.7526794                                                                                                              | Eye tracking and joy stick were used to control an assistive robotic arm over a display.                                                                                    | Tobii Rex eye tracker, Tobii                                        |Dark pupil detection with 3D model [11], Eye camera with IR illumination [12].| remote     | 3D | MICO, Kinova                                            | -                                                                                                                                                                                                                                                          | Matlab                                                                                                   | Fixation                                                                 | The task was described as picking up a tennis ball.                                                                                                                                                                                                                                                                                                             | 5 participants                                                                                 | 5                                                         | USE-questionnaire                                    |
| Wöhle, L., and Gebhard, M.                                                                                                    | 2021 | Towards Robust Robot Control in Cartesian Space Using an Infrastructureless Head- and Eye-Gaze Interface             | https://doi.org/10.3390/s21051798                                                                                                                     | Eye tracking and head tracking were used seperately to control a industrial robot. A motion capturing system was used to generate additional information in the experiment. | MARG-sensor for head position, not specified; Pupil core, Pupil Lab |Dark pupil detection with 3D model [11], Eye camera with IR illumination [12].| Head-worn  | 3D | Dual Arm UR-5, Universal Robots; gripper 2F-85, Robotiq | Selfmade, head tracking via visual markers (tilt in the video). Use of PID to control Robot (automated trajectory)                                                                                                                                         | Robot Operating System (ROS), ORB SLAM 2 for visual odometry estimation, Pupil Labs Pupil Service        | Wink, fixation                                                           | The participant randomly gazes at five different target points inside a robots working area for 20 min in total. The user blinks with the left eye to send the gaze point to the robot control pipeline upon which the robot moves to this point.                                                                                                               | 3 participants                                                                                 | 10                                                        | -                                                    |
| Yang, B., Huang, J., Sun, M., Huo, J., Li, X., Xiong, C.                                                                      | 2021 | Head-free, Human Gaze-driven Assistive Robotic System for Reaching and Grasping                                      | https://doi.org/10.23919/CCC52363.2021.9549800                                                                                                        | Eye tracking was used to control a prosthesis.                                                                                                                              | Modified Pupil Core, Pupil Labs                                     |Corneal reflections, CCD camera with IR LED| Head-worn  | 3D | Prosthesis, Not specified                               | Selfmade, servo control for obstacle avoidance by peoples fixation and selfmade 3D-estimation of gaze coordinates                                                                                                                                          | Robot Operating System (ROS),Moveit!                                                                     | Fixation, Sliding window with dwell-time 700ms                           | Experiment I: By fixating on the scissors, the robot would reach for it and bring it toward the user. Experiment II: By fixation on the scissors, the robot would reach the scissors and catch the scissors. Then plan the motion trajectory of the robotic arm through a series of fixations to avoid obstacles on the table and finally bring it to the user. | 5 participants                                                                                 | 2 tests with each 5 trials                                | QUEST                                                |
| Yoo, D.H., Kim, J.H., Kim, D.H., and Chung, M.J.                                                                              | 2002 | A Human-Robot Interface using Vision-Based Eye Gaze estimation System                                                | https://doi.org/10.1109/IRDS.2002.1043896                                                                                                             | Eyetracking was used to control a robot over a display.                                                                                                                     | CCD-Camera                                                          |Pupil center/corneal reflection and dark pupil tracking depending on the measured parameter, Backlight Assisted Near Infrared illuminators [6, 5]| Remote     | 3D | -                                                       | Self-made GUI and algorithm to evaluate glints on the cornea                                                                                                                                                                                               | -                                                                                                        | Fixation                                                                 | The user controlled the robot over a display in which he focused on buttons. The display was separated in squares to facilitate the evaluation.                                                                                                                                                                                                                 | -                                                                                              | 200 trials in total                                       | -                                                    |
| Zeng, H., Wang, Y., Wu, C., Song, A., Liu, J., Ji, P., Xu, B., Zhu, L., Li, H., and Wen, P.                                   | 2017 | Closed-Loop Hybrid Gaze Brain-Machine Interface Based Robotic Arm Control with Augmented Reality Feedback            | https://doi.org/10.3389/fnbot.2017.00060                                                                                                              | BCI and eye tracking was used to control an educational robot over a display.                                                                                               | EyeX, Tobii AB Inc.                                                 |Six dry electrodes for horizontal and vertical EOG| Remote     | 3D | Dobot, ShenzhenYuejiang TechnologyCo Inc.               | -                                                                                                                                                                                                                                                          | OpenVibe toolbox for EEG, Image Processing for Task automation. OpenCV and OpenGL for AR Feedback        | Fixation                                                                 | For each online trial, the BMI user operates the robotic arm to transfer a cuboid to the target area in the same color while avoiding the virtual obstacle in the middle of the workspace.                                                                                                                                                                      | 8 participants                                                                                 | 30 trials: 15 with and 15 without feedback                | -                                                    |

